{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install roboflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Players-Dataset-4 to yolov8:: 100%|██████████| 10491/10491 [00:13<00:00, 799.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to Players-Dataset-4 in yolov8:: 100%|██████████| 424/424 [00:00<00:00, 7282.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"cJUunyRGd3YuAM3cnXbU\")\n",
    "project = rf.workspace(\"actions-players\").project(\"players-dataset\")\n",
    "version = project.version(4)\n",
    "dataset = version.download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Players-Dataset-4/Players-Dataset-4/valid'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.move('Players-Dataset-4/train',\n",
    "            'Players-Dataset-4/Players-Dataset-4/train'\n",
    "            )\n",
    "\n",
    "shutil.move('Players-Dataset-4/test',\n",
    "            'Players-Dataset-4/Players-Dataset-4/test'\n",
    "            )\n",
    "\n",
    "shutil.move('Players-Dataset-4/valid',\n",
    "            'Players-Dataset-4/Players-Dataset-4/valid'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/khalil/VAS/training/Players-Dataset-4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning cash from gpu\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/khalil/VAS/training/Players-Dataset-4'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.40 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.25 🚀 Python-3.10.12 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce GTX 1650, 3721MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.pt, data=/home/khalil/VAS/training/Players-Dataset-4/data.yaml, epochs=20, time=None, patience=100, batch=8, imgsz=420, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train10, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train10\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3776275  ultralytics.nn.modules.head.Detect           [1, [192, 384, 576]]          \n",
      "Model summary: 295 layers, 25,856,899 parameters, 25,856,883 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 469/475 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "WARNING ⚠️ imgsz=[420] must be multiple of max stride 32, updating to [448]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/khalil/VAS/training/Players-Dataset-4/Players-Dataset-4/tr\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/khalil/VAS/training/Players-Dataset-4/Players-Dataset-4/vali\u001b[0m\n",
      "Plotting labels to runs/detect/train10/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
      "Image sizes 448 train, 448 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train10\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/20      3.64G      1.747      1.706       1.35         31        448: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646      0.576       0.69      0.633      0.248\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/20       3.5G      1.564      1.117      1.176         46        448: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646      0.461      0.738      0.517      0.242\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/20       3.5G      1.554      1.048        1.2         58        448: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646      0.115      0.709      0.106     0.0469\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/20       3.5G      1.497      1.001      1.216         32        448: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646      0.179      0.807      0.168     0.0772\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/20      3.46G      1.485     0.9784      1.209         20        448: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646      0.289       0.74      0.267      0.119\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/20      3.44G      1.575      1.004      1.251         29        448: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646      0.686      0.687      0.645      0.306\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/20      3.52G      1.587     0.9632      1.244         45        448: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646      0.562      0.677      0.546      0.262\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/20      3.45G      1.467     0.9642      1.224         22        448: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646      0.768      0.762      0.773      0.357\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/20       3.5G       1.53     0.9628      1.259         22        448: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646      0.715      0.792      0.788      0.357\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/20      3.48G      1.505     0.9235       1.19         53        448: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646      0.808      0.789       0.84      0.426\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/20      3.52G      1.475     0.8567      1.198         21        448: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646      0.841       0.81      0.872      0.436\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/20      3.44G      1.382     0.8144      1.173         24        448: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646      0.864      0.814      0.895      0.478\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/20      3.45G       1.35      0.753      1.156         20        448: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646      0.859      0.814      0.881      0.461\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/20      3.46G      1.317     0.7413       1.16         17        448: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646      0.859      0.874      0.913      0.517\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/20      3.53G      1.334     0.7679      1.166         22        448: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646      0.871      0.865       0.91      0.512\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/20      3.44G       1.29     0.7312       1.16         13        448: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646      0.865      0.864      0.912      0.508\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/20      3.47G       1.23     0.6933      1.109          9        448: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646       0.87      0.873      0.915       0.53\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/20      3.46G      1.268     0.7133      1.141         13        448: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646       0.91      0.868      0.932      0.537\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/20      3.52G      1.208     0.6594      1.082         21        448: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646      0.904      0.885      0.939      0.536\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/20      3.44G      1.203     0.6539      1.086         23        448: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646      0.914      0.886       0.94      0.534\n",
      "\n",
      "20 epochs completed in 0.068 hours.\n",
      "Optimizer stripped from runs/detect/train10/weights/last.pt, 52.0MB\n",
      "Optimizer stripped from runs/detect/train10/weights/best.pt, 52.0MB\n",
      "\n",
      "Validating runs/detect/train10/weights/best.pt...\n",
      "Ultralytics 8.3.25 🚀 Python-3.10.12 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce GTX 1650, 3721MiB)\n",
      "Model summary (fused): 218 layers, 25,840,339 parameters, 0 gradients, 78.7 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         68        646      0.909      0.868      0.932      0.537\n",
      "Speed: 0.2ms preprocess, 18.8ms inference, 0.0ms loss, 1.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train10\u001b[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/train\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect mode=train model=yolov8m.pt data={dataset.location}/data.yaml epochs=20 imgsz=420 batch=8 amp=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on valid set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = YOLO('runs/detect/train10/weights/best.pt')\n",
    "\n",
    "# Validate the model on the validation set\n",
    "metrics = model.val(data='/home/khalil/VAS/training/Players-Dataset-4/data.yaml')\n",
    "print(metrics)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model on the test set \n",
    "metrics = model.val(data='/home/khalil/VAS/training/Players-Dataset-4/data.yaml',split='test')\n",
    "print(metrics)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predections results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/khalil/VAS/training/../image.png: 256x448 12 players, 39.5ms\n",
      "Speed: 1.4ms preprocess, 39.5ms inference, 1.3ms postprocess per image at shape (1, 3, 256, 448)\n",
      "tensor([[4.3205e+02, 5.0317e+02, 4.6643e+02, 6.2034e+02, 7.6612e-01, 0.0000e+00],\n",
      "        [3.5227e+02, 5.4701e+02, 3.9729e+02, 6.8708e+02, 7.6568e-01, 0.0000e+00],\n",
      "        [4.9186e+02, 4.6543e+02, 5.3523e+02, 5.5766e+02, 7.4310e-01, 0.0000e+00],\n",
      "        [6.9974e+02, 4.2740e+02, 7.3794e+02, 5.5332e+02, 7.2838e-01, 0.0000e+00],\n",
      "        [9.1596e+02, 4.7053e+02, 9.4940e+02, 5.9985e+02, 6.3586e-01, 0.0000e+00],\n",
      "        [1.0695e+03, 4.9924e+02, 1.0970e+03, 6.2108e+02, 6.3280e-01, 0.0000e+00],\n",
      "        [8.9861e+02, 4.6801e+02, 9.3175e+02, 5.9978e+02, 5.6972e-01, 0.0000e+00],\n",
      "        [2.5026e+02, 6.0547e+02, 2.9023e+02, 7.3682e+02, 5.4193e-01, 0.0000e+00],\n",
      "        [1.5719e+03, 4.5023e+02, 1.6173e+03, 5.8258e+02, 5.3597e-01, 0.0000e+00],\n",
      "        [1.1661e+03, 6.1021e+02, 1.2247e+03, 7.8749e+02, 4.9445e-01, 0.0000e+00],\n",
      "        [7.0392e+02, 4.2806e+02, 7.5126e+02, 5.4794e+02, 3.7279e-01, 0.0000e+00],\n",
      "        [1.0767e+03, 4.9592e+02, 1.1330e+03, 6.1803e+02, 3.6375e-01, 0.0000e+00]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Run YOLO inference\n",
    "results = model('../image.png')  \n",
    "print(results[0].boxes.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### boxes plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/khalil/VAS/training/../2.png: 256x448 16 players, 15.5ms\n",
      "Speed: 15.8ms preprocess, 15.5ms inference, 1.2ms postprocess per image at shape (1, 3, 256, 448)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load the image\n",
    "image_path = '../2.png'\n",
    "frame = cv2.imread(image_path)\n",
    "\n",
    "# Convert BGR image to RGB for visualization\n",
    "frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# YOLO model inference\n",
    "results = model(image_path)\n",
    "\n",
    "# Extract the bounding boxes\n",
    "boxes = results[0].boxes.data.cpu().numpy()  # Convert to numpy array for easier handling\n",
    "\n",
    "# Iterate through the boxes and draw them\n",
    "for box in boxes:\n",
    "    x_min, y_min, x_max, y_max, confidence, class_id = box\n",
    "    x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n",
    "\n",
    "    # Draw the rectangle on the image\n",
    "    color = (0, 255, 0)  # Green for the bounding box\n",
    "    thickness = 2\n",
    "    cv2.rectangle(frame_rgb, (x_min, y_min), (x_max, y_max), color, thickness)\n",
    "\n",
    "    # Optional: Add class ID and confidence\n",
    "    label = f\"Class {int(class_id)}, {confidence:.2f}\"\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.5\n",
    "    text_thickness = 1\n",
    "    cv2.putText(frame_rgb, label, (x_min, y_min - 10), font, font_scale, color, text_thickness)\n",
    "\n",
    "# Display the image with bounding boxes using matplotlib\n",
    "plt.imshow(frame_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Optionally, save the resulting image\n",
    "cv2.imwrite(\"../output_with_boxes.png\", cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
